{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aaa46af",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbd999",
   "metadata": {},
   "source": [
    "## plots for univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e371274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "### Draw Scatter plot for numerical columns And analyse the distribution \n",
    "for column in dataset.columns :\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=dataset, x=column, y=dataset.index,hue=dataset.Price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# kde plot\n",
    "### Draw hist plot for numerical columns And analyse the distribution \n",
    "for column in dataset.columns :\n",
    "    plt.figure()\n",
    "    sns.displot(dataset[column],color='darkorange',kind='kde')\n",
    "#     sns.displot(dataset[column], color='darkorange', kind='hist')\n",
    "    \n",
    "# Observation : Here we can see other than column 'Avg. Area Number of Bedrooms' \n",
    "# all columns seems to be Normally/Gaussian/Symetrically distributed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# box plots\n",
    "### Draw box plot \n",
    "# Identify Outliers  for numerical columns. It represents the five-point summary. \n",
    "for column in dataset.columns :\n",
    "    plt.figure()\n",
    "    sns.boxplot(x = dataset[column], data = dataset, hue=column)\n",
    "\n",
    "\n",
    "# histograms\n",
    "def plot_all_histograms(df, title=\"\"):\n",
    "    numeric_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    hor = 3\n",
    "    ver = int(np.ceil(len(numeric_cols) / hor))\n",
    "    fig, axes = plt.subplots(nrows=ver, ncols=hor, figsize=(5*hor,4*ver))\n",
    "    axes = axes.flatten()\n",
    "    c = [\"r\",\"g\",\"b\",\"y\",\"r\",\"g\",\"b\",\"y\",\"r\",\"g\",\"b\",\"y\"]\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i], bins=30, color=c[i])\n",
    "        axes[i].set_title(f\"{title} {col}\")\n",
    "        axes[i].set_xlabel(\"\")\n",
    "        axes[i].set_ylabel(\"\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#for selecting numeric columns\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_cols]\n",
    "this will only take numerical columns from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212814a",
   "metadata": {},
   "source": [
    "## plots for bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "sns.pairplot(data=dataset)\n",
    "\n",
    "#coorelation and heat plot\n",
    "corr = dataset.corr()\n",
    "corr\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(corr,cmap= 'RdYlGn',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1040acd",
   "metadata": {},
   "source": [
    "## MERGE and concatinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use when you want to stack DataFrames either:\n",
    " Vertically (row-wise / one below another) → like appending rows\n",
    " Horizontally (column-wise / side by side) → like adding columns\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "# Concatenate row-wise (default axis=0)\n",
    "result = pd.concat([df1, df2])\n",
    "print(result)\n",
    "\n",
    "#concatinate column wise\n",
    "result = pd.concat([df1, df2], axis=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MERGING\n",
    "Use when you want to combine DataFrames based on a common column/key, similar to SQL JOINs.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['A', 'B', 'C']})\n",
    "df2 = pd.DataFrame({'ID': [1, 2, 4], 'Age': [25, 30, 28]})\n",
    "\n",
    "# Inner Join on column 'ID'\n",
    "result = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f758238",
   "metadata": {},
   "source": [
    "## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = dataset.isnull().sum()\n",
    "missing_value_percentages = missing_values * 100 / len(dataset)\n",
    "\n",
    "missing_value_df = pd.DataFrame(data=[missing_values, missing_value_percentages], index=[\"Total\", \"%\"]).T\n",
    "missing_value_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221faa83",
   "metadata": {},
   "source": [
    "### handel mising values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74baf630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing value \n",
    "# Let's impute missing values of column Avg. Area Income, Avg. Area Number of Rooms and Avg. Area Number of Bedrooms\n",
    "\n",
    "dataset['Avg. Area Income'] = dataset['Avg. Area Income'].fillna(dataset['Avg. Area Income'].median())\n",
    "dataset['Avg. Area Number of Rooms'] = dataset['Avg. Area Number of Rooms'].fillna(dataset['Avg. Area Number of Rooms'].mean())\n",
    "dataset['Avg. Area Number of Bedrooms'] = dataset['Avg. Area Number of Bedrooms'].fillna(dataset['Avg. Area Number of Bedrooms'].mean())\n",
    "\n",
    "# dataset['Avg. Area House Age'] = dataset['Avg. Area House Age'].fillna(dataset['Avg. Area House Age'].mean())\n",
    "# dataset['Area Population'] = dataset['Area Population'].fillna(dataset['Area Population'].mean())\n",
    "# dataset['Price'] = dataset['Price'].fillna(dataset['Price'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c9d24",
   "metadata": {},
   "source": [
    "## outlier handeling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42582215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Removal : Z-Score method\n",
    "# We will use z-score method to remove outliers as in univirate analysis we have seen our data is uniformally distributed\n",
    "print('Before outlier removal : ',dataset.shape)\n",
    "# cols = ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Area Population', 'Price']\n",
    "z = np.abs(zscore(dataset, axis = 1))\n",
    "threshold = 3\n",
    "# print(np.where(z>threshold))\n",
    "dataset = dataset[(z < threshold).all(axis=1)]\n",
    "print('After Z-Score approach : ',dataset.shape)\n",
    "\n",
    "# As we have Avg. Area Number of Bedrooms column is not normally distributed then let's use IQR method for outlier removal\n",
    "# cols = ['Avg. Area Number of Bedrooms']\n",
    "# Q1 = dataset.quantile(0.25)  # Q1\n",
    "# Q3 = dataset.quantile(0.75)  # Q3\n",
    "# IQR = Q3-Q1\n",
    "# dataset = dataset[~((dataset<(Q1-1.5*IQR)) | (dataset>(Q3+1.5*IQR)))]\n",
    "# print('After IQR approach : ',dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fb0b6",
   "metadata": {},
   "source": [
    "## Scaling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stndard scalar:\n",
    "Use when Data that roughly follows a normal (Gaussian) distribution\n",
    "Dont use If strong outliers are present\n",
    "Centers data to mean = 0, std = 1\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scalar=StandardScaler()\n",
    "scaled_data=std_scalar.fit_transform(dataset)\n",
    "scaled_df=pd.DataFrame(scaled_data,columns=dataset.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "\"\"\"\n",
    "Min Max scalar:\n",
    "use when Algorithms that require bounded input, like Neural Networks, KNN, or Gradient Descent-based models\n",
    "dont use When outliers exist — they can compress all other values\n",
    "Scales data to 0 to 1 (or custom range)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "# Initialize scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# Fit and transform\n",
    "scaled_data = minmax_scaler.fit_transform(dataset)\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=dataset.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "\"\"\"\n",
    "Robust scalar:\n",
    "Data with outliers\n",
    "dont use When you want data strictly in 0-1 range\n",
    "Uses median and IQR → less affected by outliers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "# Initialize scaler\n",
    "robust_scaler = RobustScaler()\n",
    "# Fit and transform\n",
    "scaled_data = robust_scaler.fit_transform(dataset)\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=dataset.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210b5e6",
   "metadata": {},
   "source": [
    "# Encoding techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784eb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "\"\"\"\n",
    "when we have unordered data .\n",
    "dont use in regression \n",
    "\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "dataset['State'] = encoder.fit_transform(dataset['State'])\n",
    "\n",
    "# one-hot Encoding\n",
    "\"\"\"\n",
    "can be used with unordered data\n",
    "used in regression\n",
    "\"\"\"\n",
    "dataset = pd.get_dummies(dataset, columns=['State'])\n",
    "print(dataset.head())\n",
    "\n",
    "# Ordinal encoder\n",
    "\"\"\"\n",
    "for ordered data\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
    "df['size_encoded'] = encoder.fit_transform(df[['size']])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ace63",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42302919",
   "metadata": {},
   "source": [
    "## Clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734b3c4",
   "metadata": {},
   "source": [
    "### Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS WITHOUT PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Elbow method\n",
    "wcss = []\n",
    "K = range(1, 11)   # test 1 to 10 clusters\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
    "    kmeans.fit(scaled_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K, wcss, marker=\"o\", linestyle=\"--\")\n",
    "plt.xticks(K)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (Inertia)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "#got elbox value = 2\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(scaled_df)\n",
    "labels = model.labels_\n",
    "silhouette_score(scaled_df,labels)\n",
    "\n",
    "scaled_df[\"Cluster\"] = labels\n",
    "\n",
    "df[\"Cluster\"] = labels # ADD CLUSTER TO ORIGNAL DATAFRAME\n",
    "\n",
    "features = [\"flat_type\",\"floor_area_sqm\",\"flat_model\",\"CPI Housing\",\"Adjusted_Price\"]\n",
    "\n",
    "cluster_summary = df.groupby(\"Cluster\")[features].agg([\"mean\",\"std\"]).abs().round(2)\n",
    "\n",
    "#directly calculate the valur of elbow(nclusters)\n",
    "from kneed import KneeLocator\n",
    "knee = KneeLocator(range(1,10), wcss, curve=\"convex\", direction=\"decreasing\")\n",
    "knee.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMEANS WTH PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "df_pca = pd.DataFrame(data=df_pca)\n",
    "pca.explained_variance_ratio_.sum()# This tells how much variance is covered by each columns , so that we can drop those columns which covers very less variance\n",
    "\n",
    "wcss = []\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(df_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,10), wcss)\n",
    "plt.xticks(range(1,10))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical clustering without PCA\n",
    "sampled_df = scaled_df.sample(n=5000, random_state=42)\n",
    "import scipy.cluster.hierarchy as sch\n",
    "plt.figure(figsize=(15,8))\n",
    "dendrogram = sch.dendrogram(sch.linkage(sampled_df, method=\"ward\"))\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Customers\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical clustering with PCA\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "ac.fit(df_pca)\n",
    "y_ac = ac.fit_predict(df_pca)\n",
    "silhouette_score(df_pca, y_ac)\n",
    "df3[\"class\"] = y_ac  #instead odf labels we get y_ac\n",
    "\n",
    "# Plot using first 2 PCA components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(\n",
    "    df_pca[:,0], df_pca[:,1],   # PCA components\n",
    "    c=y_ac, cmap=\"viridis\", s=30, alpha=0.7\n",
    ")\n",
    "plt.title(f\"Hierarchical Clustering with PCA (Silhouette={silhouette_score})\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDBSCSAN without PCA\n",
    "\n",
    "from sklearn.cluster import HDBSCAN\n",
    "hdbscan = HDBSCAN()\n",
    "hdbscan.fit(df_scaled)\n",
    "hdbscan.labels_\n",
    "silhouette_score(df_scaled, hdbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863aa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCSAN without PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=1.3, min_samples=9)\n",
    "\n",
    "clusters = dbscan.fit_predict(X_pca)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('DBSCAN Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN WITH PCA\n",
    "\n",
    "hdbscan.fit(df_pca)\n",
    "silhouette_score(df_pca, hdbscan.labels_)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_cluster in min_cluster_size:\n",
    "    for min_sample in min_samples:\n",
    "        hdb = HDBSCAN(min_cluster_size=min_cluster, min_samples=min_sample).fit(df_pca)\n",
    "        labels = hdb.labels_\n",
    "\n",
    "        if len(set(labels)) <= 1:\n",
    "            continue\n",
    "\n",
    "        score = silhouette_score(df_pca, labels)\n",
    "        results.append(\n",
    "            {\n",
    "                \"min_cluster_size\" : min_cluster,\n",
    "                \"min_samples\" : min_sample,\n",
    "                \"silhouette score\" : score,\n",
    "                \"n_clusters\" : len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            }\n",
    "        )\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df = result_df.sort_values(by=\"silhouette score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e653f1",
   "metadata": {},
   "source": [
    "## Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "ac.fit(df_scaled)\n",
    "y_ac = ac.fit_predict(df_scaled)\n",
    "y_ac\n",
    "\n",
    "silhouette_score(df_scaled, y_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db70329",
   "metadata": {},
   "source": [
    "### agglomerative clustering with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "ac.fit(df_pca)\n",
    "y_ac = ac.fit_predict(df_pca)\n",
    "silhouette_score(df_pca, y_ac)\n",
    "\n",
    "\n",
    "df3 = df_main\n",
    "df3[\"class\"] = y_ac\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(df3, x=\"class\", y=\"child_mort\", hue=\"class\", palette=\"tab10\")\n",
    "plt.title(\"child_mort vs class\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(df3, x=\"class\", y=\"income\", hue=\"class\", palette=\"tab10\")\n",
    "plt.title(\"income vs class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33853e3e",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062683c",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scaled_df_new.drop([\"Adjusted_Price\"],axis =1)\n",
    "y = scaled_df_new[\"Adjusted_Price\"]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,train_size = 0.75 , random_state = 42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train,y_train)\n",
    "y_train_predict = lr_model.predict(x_train)\n",
    "y_test_predict = lr_model.predict(x_test)\n",
    "\n",
    "\n",
    "print(r2_score(y_train, y_train_predict))\n",
    "print(\"\\n\")\n",
    "print(r2_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5ff85",
   "metadata": {},
   "source": [
    "## Polynomial Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00894e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression (degree=2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(x)\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y)\n",
    "y_poly_pred = poly_reg.predict(X_poly)\n",
    "\n",
    "print(\"Polynomial Regression R2:\", r2_score(y, y_poly_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04054375",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "ridge = Ridge()\n",
    "alpha_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'alpha': alpha_space}\n",
    " \n",
    "reg_cv = GridSearchCV(ridge, param_grid, cv=5)\n",
    "reg_cv.fit(x_train, y_train)\n",
    "\n",
    "# for Ridge regression\n",
    "y_predict_train_cv = reg_cv.predict(x_train)\n",
    "y_predict_test_cv = reg_cv.predict(x_test)\n",
    "print(r2_score(y_train, y_predict_train_cv))\n",
    "print(\"\\n\")\n",
    "print(r2_score(y_test, y_predict_test_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3db25",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Create Lasso model with a fixed alpha\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lasso_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = lasso_model.predict(x_train)\n",
    "y_pred_test = lasso_model.predict(x_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Train R² score:\", r2_score(y_train, y_pred_train))\n",
    "print(\"Test R² score:\", r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d13e9",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor    \n",
    "dt_model = DecisionTreeRegressor()    \n",
    "dt_model.fit(x_train, y_train)    \n",
    "y_predict_dt_train = dt_model.predict(x_train)  \n",
    "y_predict_dt_test = dt_model.predict(x_test)\n",
    "print(r2_score(y_train, y_predict_dt_train))\n",
    "print(\"\\n\") \n",
    "print(r2_score(y_test, y_predict_dt_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71772019",
   "metadata": {},
   "source": [
    "### decision tree-randomised search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c526d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define model\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}   \n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(A\n",
    "    estimator=dt,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,           # Try 30 random combinations\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt_random = random_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt_random.predict(x_train)\n",
    "y_predict_test = best_dt_random.predict(x_test)\n",
    "\n",
    "# Scores\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3aba4",
   "metadata": {},
   "source": [
    "### decision tree-Grid search cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt.predict(x_train)\n",
    "y_predict_test = best_dt.predict(x_test)\n",
    "\n",
    "# Scores\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438c715",
   "metadata": {},
   "source": [
    "## Random forest-Grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestRegressor(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [3,12,5,1],\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt.predict(x_train)\n",
    "y_predict_test = best_dt.predict(x_test)\n",
    "\n",
    "# Scores\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0ced8",
   "metadata": {},
   "source": [
    "## Random forest-Random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define model\n",
    "dt = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators':[3,12,5,1],\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,           # Try 30 random combinations\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt_random = random_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt_random.predict(x_train)\n",
    "y_predict_test = best_dt_random.predict(x_test)\n",
    "\n",
    "# Scores\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01e689",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438c2ab",
   "metadata": {},
   "source": [
    "### ADA boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "abr = AdaBoostRegressor(n_estimators=100, learning_rate=0.05, loss=\"square\", random_state=42)\n",
    "\n",
    "abr.fit(x_train,y_train)\n",
    "\n",
    "y_predict_train = abr.predict(x_train)\n",
    "\n",
    "y_predict_test = abr.predict(x_test)\n",
    "\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffad0b",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(loss='absolute_error',\n",
    "                                learning_rate=0.8,\n",
    "                                n_estimators=500,\n",
    "                                max_depth = 6, max_features = 6)\n",
    "gbr.fit(x_train,y_train)    \n",
    "y_predict_train = gbr.predict(x_train)  \n",
    "y_predict_test = gbr.predict(x_test)  \n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))  \n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98606d69",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBRegressor\n",
    "model =  XGBRegressor() \n",
    "model.fit(x_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "print(\"R2 score Train: \",r2_score(y_train, y_pred_train))\n",
    "print(\"R2 score Test: \",r2_score(y_test, y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78c560",
   "metadata": {},
   "source": [
    "### light LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb=lgb.LGBMRegressor(learning_rate=2,\n",
    "                    n_estimators=10,\n",
    "                    max_depth = 15,\n",
    "                    # min_split_loss=0,       \n",
    "                    subsample=0.9,\n",
    "                    colsample_bytree=0.7,\n",
    "                    )\n",
    "\n",
    "\n",
    "lgb.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "y_predict_train = gbr.predict(x_train)\n",
    "y_predict_test = gbr.predict(x_test)\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967525ba",
   "metadata": {},
   "source": [
    "### CAD boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42caf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "ctb=CatBoostRegressor(loss_function='RMSE')\n",
    "ctb.fit(x_train,y_train)\n",
    "\n",
    "y_predict_train = ctb.predict(x_train)\n",
    "y_predict_test = ctb.predict(x_test)\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8bd87",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model = SVR(kernel='rbf')\n",
    "model.fit(x, y)\n",
    "y_pred_test = model.predict(x_test)\n",
    "print(\"Test R2 :\", r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745563b1",
   "metadata": {},
   "source": [
    "### KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f651cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=4,weights='uniform',algorithm=\"ball_tree\",leaf_size=500,p=1)\n",
    "knn_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_predict_train = knn_regressor.predict(x_train)\n",
    "y_predict_test = knn_regressor.predict(x_test)\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1f357",
   "metadata": {},
   "source": [
    "### Hyperparameter (grid search) on gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "gbr= GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "\n",
    "grid = {\n",
    "    'n_estimators' : [1,5,10,15,20],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'subsample' : [0.5, 0.7, 1.0],\n",
    "    'max_depth' : [3, 7, 9,12]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=grid, n_jobs=-1, cv=cv)\n",
    "\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "best_dt_gbr = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt_gbr.predict(x_train)\n",
    "y_predict_test = best_dt_gbr.predict(x_test)\n",
    "\n",
    "# Scores\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8f1db",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dd317",
   "metadata": {},
   "source": [
    "## LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1408ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_x_train = scaler.fit_transform(x_train)\n",
    "scaled_x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "scaled_x_train = pd.DataFrame(scaled_x_train,columns=x_train.columns)\n",
    "scaled_x_test = pd.DataFrame(scaled_x_test,columns=x_test.columns)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_model = LogisticRegression()  \n",
    "lr_model.fit(scaled_x_train, y_train)\n",
    "\n",
    "\n",
    "y_predict_lr_train = lr_model.predict(scaled_x_train) \n",
    "y_predict_lr_test = lr_model.predict(scaled_x_test) \n",
    "\n",
    "\n",
    "print(accuracy_score(y_train, y_predict_lr_train))   \n",
    "print(\"\\n\")\n",
    "print(accuracy_score(y_test, y_predict_lr_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4594596",
   "metadata": {},
   "source": [
    "## svm classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "svm = SVC(kernel='rbf', C=2, random_state=42)   # types of kernel{'sigmoid', 'rbf', 'precomputed', 'linear', 'poly'}\n",
    "\n",
    "# Train the classifier \n",
    "svm.fit(scaled_x_train, y_train) \n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(scaled_x_test) \n",
    "\n",
    "# Evaluate the model \n",
    "print(\"\\nAccuracy Scores:\\n\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309e539",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier with Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a127e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "dtr = DecisionTreeClassifier(max_depth=10) \n",
    "\n",
    "abr = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=42) \n",
    "\n",
    "abr.fit(scaled_x_train,y_train) \n",
    "\n",
    "y_predict_train = abr.predict(scaled_x_train) \n",
    "\n",
    "y_predict_test = abr.predict(scaled_x_test) \n",
    "\n",
    "print(\"Train R2 :\", accuracy_score(y_train, y_predict_train)) \n",
    "\n",
    "print(\"Test R2 :\", accuracy_score(y_test, y_predict_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70c9ed",
   "metadata": {},
   "source": [
    "RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': [3, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}   \n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,           # Try 30 random combinations\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(scaled_x_train, y_train) \n",
    "\n",
    "# Best model\n",
    "best_dt_random = random_search.best_estimator_ \n",
    "\n",
    "# Predict\n",
    "y_predict_train = best_dt_random.predict(scaled_x_train)\n",
    "y_predict_test = best_dt_random.predict(scaled_x_test)\n",
    "\n",
    "# Scores\n",
    "print(\"Train R2:\", accuracy_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", accuracy_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163fcd",
   "metadata": {},
   "source": [
    "### KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1000, 1100)\n",
    "\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, random_state=42).fit(X_train_scaled)\n",
    "    \n",
    "    distortions.append(sum(np.min(cdist(X_train_scaled, kmeanModel.cluster_centers_, 'euclidean'), axis=1)**2) / X.shape[0])\n",
    "    \n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "    \n",
    "    mapping1[k] = distortions[-1]   \n",
    "    mapping2[k] = inertias[-1]\n",
    "    print(k,end=\" \")\n",
    "\n",
    "print(\"Distortion values:\")\n",
    "for key, val in mapping1.items():\n",
    "    print(f'{key} : {val}')\n",
    "\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1096)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier    \n",
    "# kn=KNeighborsClassifier(15)     \n",
    "# kn.fit(scaled_x_train, y_train)      \n",
    "# y_pred=kn.predict(x_test)      \n",
    "# print(\"Accuracy Train:\", accuracy_score(y_train, kn.predict(scaled_x_train)))      \n",
    "# print(\"Accuracy Test:\", accuracy_score(y_test, y_pred))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfc54f",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412a05",
   "metadata": {},
   "source": [
    "### criterion - gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "clf_gini.fit(X_train_scaled, y_train)\n",
    "y_pred = clf_gini.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e515b9",
   "metadata": {},
   "source": [
    "### criterion - entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_ent = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=0)\n",
    "clf_ent.fit(X_train_scaled, y_train)\n",
    "y_pred = clf_ent.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e0421",
   "metadata": {},
   "source": [
    "### criterion log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b39dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_ll = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=3, random_state=0)\n",
    "clf_ll.fit(X_train_scaled, y_train)\n",
    "y_pred = clf_ll.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c580e7",
   "metadata": {},
   "source": [
    "## Naive bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59573106",
   "metadata": {},
   "source": [
    "### gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gauss_model=GaussianNB()\n",
    "gauss_model.fit(X_train_scaled,y_train)\n",
    "y_pred = gauss_model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a0fcc",
   "metadata": {},
   "source": [
    "### multinomial NB\n",
    "cant use when we have negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15566947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# gauss_model=MultinomialNB()\n",
    "# gauss_model.fit(X_train_scaled,y_train)\n",
    "# y_pred = lr.predict(X_test_scaled)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedc527",
   "metadata": {},
   "source": [
    "### Bernaullie NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bern_model=BernoulliNB()\n",
    "bern_model.fit(X_train_scaled,y_train)\n",
    "y_pred = bern_model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e764b",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11965ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=1000,max_depth=8,max_features=12)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb9b21",
   "metadata": {},
   "source": [
    "## XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgc=xgb.XGBClassifier(learning_rate=0.5,\n",
    "                    n_estimators=10,\n",
    "                    max_depth = 15)\n",
    "\n",
    "xgc.fit(X_train_scaled,y_train)\n",
    "y_pred = xgc.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce48b0a",
   "metadata": {},
   "source": [
    "## Light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgm\n",
    "lgc=lgm.LGBMClassifier(learning_rate=0.01,\n",
    "                    n_estimators=600,\n",
    "                    max_depth = 10)\n",
    "\n",
    "lgc.fit(X_train_scaled,y_train)\n",
    "y_pred = lgc.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96f99f",
   "metadata": {},
   "source": [
    "## Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc=GradientBoostingClassifier(loss=\"exponential\", # {'exponential', 'log_loss'}\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=600,\n",
    "                                max_depth = 10,\n",
    "                                max_features = 5)\n",
    "\n",
    "gbc.fit(X_train_scaled,y_train)\n",
    "y_pred = gbc.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33335797",
   "metadata": {},
   "source": [
    "## ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc=AdaBoostClassifier( n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "abc.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred = abc.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff9938",
   "metadata": {},
   "source": [
    "## cat boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cbc=CatBoostClassifier(loss_function='Logloss') # [ Logloss, CrossEntropy, MultiClass, MultiClassOneVsAll ]\n",
    "cbc.fit(X_train_scaled,y_train)\n",
    "y_pred = cbc.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39c296",
   "metadata": {},
   "source": [
    "## Extra tree clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83798bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n",
    "                                        criterion ='entropy', max_features = 2)\n",
    "\n",
    "# Training the model\n",
    "extra_tree_forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Computing the importance of each feature\n",
    "feature_importance = extra_tree_forest.feature_importances_\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e3b5d",
   "metadata": {},
   "source": [
    "## hyperparameter tuning grid search cv gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"log_loss\"],    #{'exponential', 'log_loss'}\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 2),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 2),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"squared_error\",\"friedman_mse\"],\n",
    "    \"subsample\":[0.5, 0.618],\n",
    "    \"n_estimators\":[5]\n",
    "    }\n",
    "\n",
    "gsv_gbc=GridSearchCV(GradientBoostingClassifier(),parameters,refit=True,cv=2,n_jobs=1)\n",
    "gsv_gbc.fit(X_train_scaled,y_train)\n",
    "best_dt = gsv_gbc.best_estimator_\n",
    "y_pred = best_dt.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity = cm[0,0] / (cm[0,0] + cm[0,1])  \n",
    "print(\"Specificity -> \",specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11cf5b",
   "metadata": {},
   "source": [
    "EDA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and handle null values\n",
    "\n",
    "\n",
    "# 1. Check for null values\n",
    "print(\"\\nNull values count:\\n\", df.isnull().sum())\n",
    "\n",
    "# 2. Drop rows with any null values\n",
    "df_drop_rows = df.dropna()\n",
    "print(\"\\nAfter dropping rows with nulls:\\n\", df_drop_rows)\n",
    "\n",
    "# 3. Drop columns with any null values\n",
    "df_drop_cols = df.dropna(axis=1)\n",
    "print(\"\\nAfter dropping columns with nulls:\\n\", df_drop_cols)\n",
    "\n",
    "# 4. Fill null values with a constant\n",
    "df_fill_const = df.fillna(\"Unknown\")\n",
    "print(\"\\nAfter filling with constant:\\n\", df_fill_const)\n",
    "\n",
    "# 5. Fill null values with mean/median/mode\n",
    "df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)    # Fill Age with mean\n",
    "df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)  # Fill Salary with median\n",
    "print(\"\\nAfter filling numerical nulls:\\n\", df)\n",
    "\n",
    "# 6. Forward fill (propagate last valid value forward)\n",
    "df_ffill = df.fillna(method=\"ffill\")\n",
    "print(\"\\nForward fill:\\n\", df_ffill)\n",
    "\n",
    "# 7. Backward fill (use next valid value)\n",
    "df_bfill = df.fillna(method=\"bfill\")\n",
    "print(\"\\nBackward fill:\\n\", df_bfill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05632c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if a columns contains diffferent datatypes     \n",
    "scaled_df[\"Longitude\"].apply(type).value_counts()      \n",
    "\n",
    "scaled_df[\"Longitude\"] = pd.to_numeric(scaled_df[\"Longitude\"], errors=\"coerce\")     \n",
    "\n",
    "scaled_df = scaled_df.dropna(subset=[\"Longitude\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "df = pd.get_dummies(df , columns = [\"Geography\",\"Card Type\",\"Gender\"],dtype = int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85514835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.rename(columns = {\"Gender_Female\" : \"Female\" , \"Gender_Male\" : \"Male\" , \"Geography_France\" : \"France\", \"Geography_Germany\" : \"Germany\", \"Geography_Spain\" : \"Spain\",\"Card Type_DIAMOND\" : \"Diamond\",\"Card Type_GOLD\" : \"Gold\",\"Card Type_PLATINUM\" : \"Platinum\" },inplace = True)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile (to handle higher values of skewness and kurtosis)\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "quantile_transformer = QuantileTransformer(output_distribution ='normal')\n",
    "rain_df[\"Rainfall\"] = quantile_transformer.fit_transform(rain_df[\"Rainfall\"].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f64dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zscore\n",
    "columns  = my_df.columns\n",
    "print('Before outlier removal : ',my_df.shape)\n",
    "numeric_cols = my_df.select_dtypes(include=[np.number]).columns\n",
    "z= my_df[numeric_cols].apply(zscore)\n",
    "threshold = 3\n",
    "my_df = my_df[(z < threshold).all(axis=1)]\n",
    "print('After Z-Score approach : ',my_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually adding null values\n",
    "df.loc[df.sample(100).index ,\"Balance\"] = np.nan   \n",
    "\n",
    "#selecting only numeric columns\n",
    "numeric_cols = list(df.select_dtypes(include = np.number).columns)\n",
    "\n",
    "# Convert column names to lowercase and replace spaces with underscores\n",
    "clean_dataset.columns = clean_dataset.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into datetime type\n",
    "from datetime import datetime\n",
    "df1[\"month\"] = pd.to_datetime(df1[\"month\"])\n",
    "df1[\"Tranc_Yr\"] = df1[\"month\"].dt.year\n",
    "df1[\"Tranc_Mth\"] = df1[\"month\"].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc398332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge \n",
    "merged_df = pd.merge(df1,df2,on=[\"Tranc_Yr\",\"Tranc_Mth\"],how = \"left\")1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38665222",
   "metadata": {},
   "source": [
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplot\n",
    "\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "#Visualizing using Seaborn\n",
    "plt.figure(figsize=(25,60))\n",
    "for i, col in enumerate(df[num_cols].columns, 1):\n",
    "    plt.subplot(10, 2, i)\n",
    "    sns.histplot(data=df, x=col, kde=True, bins=20, color=palette[3])\n",
    "    plt.title(f\"Histogram and Distribution for {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Visualizing Boxplots using Seaborn for numerical data\n",
    "\n",
    "plt.figure(figsize=(25,60))\n",
    "for i, col in enumerate(df[num_cols].columns, 1):\n",
    "    plt.subplot(10, 2, i)\n",
    "    sns.boxplot(y=col, data=df)\n",
    "    plt.title(f\"Boxplot for {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a221b0",
   "metadata": {},
   "source": [
    "# HSV in OPENCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efe471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: HSV Color Range Extraction & Manipulation in OpenCV\n",
    " \n",
    "# Problem Statement:\n",
    "# Write a Python program using OpenCV that:\n",
    " \n",
    "# Reads an image.\n",
    " \n",
    "# Converts it from BGR to HSV color space.\n",
    " \n",
    "# Allows the user to extract and display only a specific color range (for example, blue objects).\n",
    " \n",
    "# Lets the user manipulate the saturation and value (brightness) of the extracted color\n",
    "# region using trackbars in real-time.\n",
    " \n",
    " \n",
    " \n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "def nothing(x):\n",
    "    pass\n",
    " \n",
    "# --- Load Image ---\n",
    "img_path = r\"D:\\python_full_learning_final_1\\open_cv\\naruto.jpg\"  \n",
    "img = cv2.imread(img_path)\n",
    " \n",
    "if img is None:\n",
    "    raise FileNotFoundError(\" Image not found. Check the path.\")\n",
    " \n",
    "img = cv2.resize(img, (640, 480))\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    " \n",
    "# --- Create a window for trackbars ---\n",
    "cv2.namedWindow(\"Trackbars\")\n",
    " \n",
    "# --- Create Trackbars for lower and upper HSV range ---\n",
    "cv2.createTrackbar(\"LH\", \"Trackbars\", 0, 179, nothing)\n",
    "cv2.createTrackbar(\"LS\", \"Trackbars\", 0, 255, nothing)\n",
    "cv2.createTrackbar(\"LV\", \"Trackbars\", 0, 255, nothing)\n",
    "cv2.createTrackbar(\"UH\", \"Trackbars\", 179, 179, nothing)\n",
    "cv2.createTrackbar(\"US\", \"Trackbars\", 255, 255, nothing)\n",
    "cv2.createTrackbar(\"UV\", \"Trackbars\", 255, 255, nothing)\n",
    " \n",
    "print(\"Adjust the HSV sliders to isolate a specific color. Press ESC to exit.\")\n",
    " \n",
    "while True:\n",
    "    # --- Get current positions of all trackbars ---\n",
    "    lh = cv2.getTrackbarPos(\"LH\", \"Trackbars\")\n",
    "    ls = cv2.getTrackbarPos(\"LS\", \"Trackbars\")\n",
    "    lv = cv2.getTrackbarPos(\"LV\", \"Trackbars\")\n",
    "    uh = cv2.getTrackbarPos(\"UH\", \"Trackbars\")\n",
    "    us = cv2.getTrackbarPos(\"US\", \"Trackbars\")\n",
    "    uv = cv2.getTrackbarPos(\"UV\", \"Trackbars\")\n",
    " \n",
    "    # --- Define lower and upper HSV limits ---\n",
    "    lower = np.array([lh, ls, lv])\n",
    "    upper = np.array([uh, us, uv])\n",
    " \n",
    "    # --- Threshold the HSV image to get only desired colors ---\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    " \n",
    "    # --- Bitwise-AND mask and original image ---\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    " \n",
    "    # --- Optional: Increase brightness or saturation slightly ---\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    s = cv2.add(s, 20)  # Increase saturation\n",
    "    v = cv2.add(v, 30)  # Increase brightness\n",
    "    enhanced_hsv = cv2.merge((h, s, v))\n",
    "    enhanced_bgr = cv2.cvtColor(enhanced_hsv, cv2.COLOR_HSV2BGR)\n",
    " \n",
    "    # --- Display images ---\n",
    "    cv2.imshow(\"Original\", img)\n",
    "    cv2.imshow(\"Mask\", mask)\n",
    "    cv2.imshow(\"Filtered Color\", result)\n",
    "    cv2.imshow(\"Enhanced Image\", enhanced_bgr)\n",
    " \n",
    "    # --- Exit when ESC is pressed ---\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    " \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2587df",
   "metadata": {},
   "source": [
    "## contours in open CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img=cv2.imread('shapes.jpg')\n",
    "img=cv2.resize(img,(300,300))\n",
    "\n",
    "grey=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_,thres=cv2.threshold(grey,200,255,cv2.THRESH_BINARY)\n",
    "\n",
    "contours,hierarchy= cv2.findContours(thres,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "\n",
    "for conto in  contours:\n",
    "    approx=cv2.approxPolyDP(conto,0.01*cv2.arcLength(conto,True),True)\n",
    "    corners=len(approx)\n",
    "    # print(corners)\n",
    "    if corners==3:\n",
    "        shape_name=\"Triangle\"\n",
    "    elif corners==4:\n",
    "        shape_name=\"rectangle\"\n",
    "    elif corners==5:\n",
    "        shape_name=\"pentagon\"\n",
    "    else:\n",
    "        shape_name=\"circle\"\n",
    "    cv2.drawContours(img,[approx],-1,(0,255,0),2)\n",
    "    x=approx.ravel()[0]\n",
    "    print(x)\n",
    "    y=approx.ravel()[1]-10\n",
    "    print(y)\n",
    "    print('-----------------')\n",
    "    cv2.putText(img,shape_name,(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(255,0,0))\n",
    "\n",
    "\n",
    "cv2.imshow(\"orig\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
